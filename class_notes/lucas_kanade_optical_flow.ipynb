{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucas Kanade Optical Flow\n",
    "\n",
    "## Motivation\n",
    "* Optical flow is the pattern of motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene\n",
    "* It is a 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second\n",
    "\n",
    "## Applications\n",
    "Optical flow has many application domains:\n",
    "* Structure from motion\n",
    "* Video compression\n",
    "* Video stabilization\n",
    "* Autonomous vehicles\n",
    "* Object detection and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundational Ideas\n",
    "It works on several assumptions:\n",
    "* The pixel intensities of an object do not change between consecutive frames\n",
    "* Neighbouring pixels have similar motion\n",
    "* It assumes that motion is smooth, which means that the motion of one object is not independent of the motion of its neighbours\n",
    "\n",
    "# Images as 3D objects\n",
    "* Previously we've talked about how although images are 2D arrays of pixels, they are better thought of as samples from a continuous image surface: $I(x, y)$\n",
    "* We've also talked about other dimensions that we could consider such as\n",
    "    * Scale\n",
    "    * Color\n",
    "    * Time\n",
    "* For now, we'll focus on the time dimension\n",
    "* This means we can think of a sequence of frames (video) as samples of a 3D function: $I(x, y, t)$\n",
    "\n",
    "# Optical Flow Foundational Equation\n",
    "* Suppose the frames are capturing a moving object\n",
    "* This means from one frame to another, the image will be displaced by some amount\n",
    "    * We considered this idea previously in the context of Harris corner detection\n",
    "* The object is also moving through the frame over time\n",
    "* Suppose our object moves $(dx, dy)$ in a time interval of $dt$\n",
    "* Assuming the intensity doesn't change and the movement is purely translational, we can write the following equation:\n",
    "\\begin{equation*}\n",
    "    I(x + dx, y + dy, t + dt) = I(x, y, t) \n",
    "    \\hspace{100em}\n",
    "\\end{equation*}\n",
    "If we then take the Taylor expansion of the right hand side, we get:\n",
    "\\begin{align*}\n",
    "    I(x + dx, y + dy, t + dt) &\\approx I(x, y, t) + \\frac{\\partial I}{\\partial x}dx + \\frac{\\partial I}{\\partial y}dy + \\frac{\\partial I}{\\partial t}dt\n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "which means\n",
    "\\begin{align*}\n",
    "    I(x + dx, y + dy, t + dt) &= I(x, y, t) \\\\\n",
    "    I(x + dx, y + dy, t + dt) - I(x, y, t) &= 0 \\\\\n",
    "    \\frac{\\partial I}{\\partial x}dx + \\frac{\\partial I}{\\partial y}dy + \\frac{\\partial I}{\\partial t}dt &= 0\\\\\n",
    "    \\frac{\\partial I}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial I}{\\partial y} \\frac{dy}{dt}  + \\frac{\\partial I}{\\partial t} &=0\\\\\n",
    "    \\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} &=0\n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "where $u \\triangleq \\frac{dx}{dt}$ and $v \\triangleq \\frac{dy}{dt}$ are the velocities in the $x$ and $y$ directions respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucas-Kanade Method\n",
    "\n",
    "### History\n",
    "* Original paper was published in 1981!\n",
    "* Ideas are still used today\n",
    "* Paper was about image registration, with a focus on stereo matching for depth estimation\n",
    "* The method was later adapted for optical flow estimation (1981)\n",
    "\n",
    "### Original Paper Ideas\n",
    "\n",
    "* Goal: Estimate the displacement of a patch between two frames\n",
    "* Considered three measures of match\n",
    "    * $L_1$ norm\n",
    "    * $L_2$ norm\n",
    "    * Cross correlation\n",
    "* Argued, weirdly, that $L_1$ is an \"inexpensive\" approximation to the $L_2$ norm\n",
    "\n",
    "#### Previous Ideas\n",
    "- Exhaustive search\n",
    "    - Limitations\n",
    "        - Ridiculously expensive (computational cost)\n",
    "    - If image is $N \\times N$ and the search region is $M \\times M$, then the cost is $\\mathcal{O}(N^2M^2)$\n",
    "- Hill-climbing \n",
    "    - Start with an initial guess and iteratively improve\n",
    "    - Perhaps each iterative search is small (say $3 \\times 3$), but you may have to do this many times\n",
    "    - Limitations\n",
    "        - Unclear how to choose the initial guess\n",
    "        - Has a tendency to get stuck in local minima (false peaks)\n",
    "    - Claimed, without evidence, requires $\\mathcal{O}(M^2 N)$ operations\n",
    "- Sequential Similarity Detection\n",
    "    - When calculating the sum of a norm, if it exceeds the best match so far, stop\n",
    "- Coarse-Fine Search\n",
    "    - Basically an image pyramid\n",
    "    - Start at a low resolution and work your way up\n",
    "    - Low resolution match constrains the region of matches at the next level of higher resolution\n",
    "- Combinations\n",
    "    - Most of the above methods can be combined\n",
    "\n",
    "#### Lucas-Kanade Key Idea In One Dimension\n",
    "- Perhaps best understood with one-dimensional example\n",
    "- Suppose we have $g(x)$ and wish to match it to $f(x + h)$ for some $h$\n",
    "- We can use the Taylor expansion to approximate $f(x + h)$\n",
    "\\begin{align*}\n",
    "    f(x + h) &\\approx f(x) + f'(x)h + \\ldots\n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "- We can then minimize the $L_2$ norm of the error\n",
    "\\begin{align*}\n",
    "    E(h) &\\triangleq \\sum_{x \\in R} \\left(f(x + h) - g(x)\\right)^2 \\\\\n",
    "    &\\approx \\sum_{x \\in R}  \\left(f(x) + f'(x)h - g(x)\\right)^2 \\\\\n",
    "    \\frac{d E(h)}{d h}\n",
    "    &= 2 \\sum_{x \\in R} \\left(f(x) + f'(x)h - g(x)\\right) f'(x) \\\\\n",
    "    \\sum_{x \\in R} \\left(f(x) + f'(x)\\hat{h} - g(x)\\right) f'(x) &= 0 \\\\\n",
    "    \\sum_{x \\in R} f'(x) \\left(f(x) + f'(x)\\hat{h} - g(x)\\right) &= 0 \\\\\n",
    "    \\hat{h} &= \\frac{\\sum_{x \\in R} f'(x) \\left(g(x) - f(x)\\right)}{\\sum_{x \\in R} f'(x)^2}\n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lucas-Kanade Key Idea In Two Dimensions\n",
    "- If the displacement is from one frame to another in a video sequence, then the displacement is effectively a velocity vector\n",
    "    - It represents the change in position over the inter-frame interval $dt$    \n",
    "- Let the velocities be represented as $u$ and $v$ that satisfy the optical flow equation in that window\n",
    "\n",
    "Let's assume we wish to minize the $L_2$ norm of the error between one patch in a frame and a shifted patch in the next frame. We'll use the first order Taylor expansion to approximate the shifted patch. We'll also assume that the motion is small, so we can ignore the higher order terms. This gives us the following equation:\n",
    "\\begin{align*}\n",
    "    I(x+u, y+v, t) &\\approx I(x, y, t) + \\frac{\\partial I(x,y,t)}{\\partial x}dx + \\frac{\\partial I(x,y,t)}{\\partial y}dy \\\\\n",
    "    E(h) &\\triangleq \\sum_{x \\in R} \\left(I(x+u, y+v, t) - I(x, y, t + dt)\\right)^2 \\\\\n",
    "    &\\approx \\sum_{x \\in R}  \\left(I(x, y, t) + \\frac{\\partial I(x,y,t)}{\\partial x} u + \\frac{\\partial I(x,y,t)}{\\partial y} v - I(x, y, t + dt)\\right)^2 \n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "where $h \\triangleq [u, v]^T$ is the vector of velocities (changes from one frame to the next). The gradient of $E(h)$ is:\n",
    "\\begin{align*}\n",
    "    \\nabla_h E(h) &\\triangleq \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial E}{\\partial u} \\\\\n",
    "        \\frac{\\partial E}{\\partial v}\n",
    "    \\end{bmatrix} \\\\\n",
    "    &= 2 \\sum_{x,y \\in R} \\left(I(x, y, t) + \\frac{\\partial I(x,y,t)}{\\partial x} u + \\frac{\\partial I(x,y,t)}{\\partial y} v - I(x, y, t + dt)\\right) \\nabla_{x,y} I(x,y,t) \\\\\n",
    "    &= 2 \\sum_{x,y \\in R} \\nabla_{x,y} I(x,y,t) \\left(I(x, y, t) + (\\nabla_{x,y} I(x,y,t))^T h - I(x, y, t + dt)\\right) \\\\\n",
    "    &= 0\n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "which means if we let $g \\triangleq \\nabla_{x,y} I(x,y,t)$ represent the image gradient (column) vector, we can solve for $h$ as follows:\n",
    "\\begin{align*}\n",
    "    \\sum_{x,y \\in R} g \\left(I(x, y, t) + g^T h - I(x, y, t + dt)\\right)  &= 0 \\\\\n",
    "    \\left(\\sum_{x,y \\in R} g \\left(g^T \\right)\\right) h &= \\sum_{x,y \\in R} g \\left(I(x, y, t + dt) - I(x, y, t)\\right) \\\\\n",
    "    h &= \\left(\\sum_{x,y \\in R} \\left(g g^T \\right)^{-1}\\right) \\sum_{x,y \\in R} g \\left(I(x, y, t + dt) - I(x, y, t)\\right)\n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "\n",
    "* Note that like the Harris corner detector, this is driven by the average outer product of the Image gradient\n",
    "* However, unlike the Harris corner detector, we're solving for the most accurate displacement of the patch in the next frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Improvements\n",
    "- Spatial weighting\n",
    "    - Typically Gaussian\n",
    "- Acount for affine transforms\n",
    "    - This would require a 6D vector instead of a 2D vector\n",
    "- Pyramidal approach\n",
    "    - This would involve solving for the optical flow at multiple scales\n",
    "    - This would allow for larger displacements to be captured\n",
    "- Account for changes in lighting \n",
    "    - Brightness (additive) \n",
    "    - Contrast (multiplicative)\n",
    "- Do iterative improvements\n",
    "    - This would involve solving for the optical flow, then warping the image and solving again\n",
    "    - This would be repeated until convergence or you reach the limit of your computational budget \n",
    "\n",
    "For example, even in 1981, the authors suggested the following to account for affine transformations and changes in brightness and contrast:\n",
    "\\begin{align*}\n",
    "    E(h) &\\triangleq \\sum_{x \\in R} \\left(I(xA + h) - \\left(\\alpha I(x, y, t + dt) + \\beta\\right) \\right)^2 \n",
    "    \\hspace{100em}\n",
    "\\end{align*}\n",
    "where $x$ here is a 2D vector representing the spatial location, $A$ is a $2 \\times 2$ matrix, and $\\alpha$ and $\\beta$ are the brightness and contrast changes respectively\n",
    "- Note that affine transformations are a generalization of translations, rotations, and scaling\n",
    "- However, writing the equation and solving it are two different things\n",
    "    - The authors did not propose a method for estimating $A$, $\\alpha$, and $\\beta$ along with $h$\n",
    "    - They did note that if we \"ignore A\", the problem is equivalent to maximizing the correlation coefficient between the two images\n",
    "        - However, this is just a problem statement, and not a solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Registration to Optical Flow\n",
    "* The original paper was about image registration\n",
    "* In 1981, the Tomasi and Kanade wrote a technical report about using the Lucas-Kanade method for optical flow\n",
    "* The idea is to use the Lucas-Kanade method to estimate the displacement of a patch from one frame to the next\n",
    "* However, this leaves an open problem: which patches should you track?\n",
    "* Their answer, in principle, was that good features are those that can be tracked well\n",
    "* They reduced this to meaning that the registration problem can be \"solved reliably\"\n",
    "* Which they then claimed meant that the problem was \"well conditioned\"\n",
    "* And, finally, they then claimed means that __both of the eigenvalues__ of the gradient outer product are \"large\"\n",
    "    * You've heard this before\n",
    "    * This is the same thinking that led to the Harris corner detector\n",
    "    * Oddly, they didn't cite the Harris paper\n",
    "* How do you pick a threshold for the eigenvalues?\n",
    "    * They measured the eigenvalues in uniform brightness\n",
    "    * This producd a lower bound\n",
    "        * The threshold has to be larger than this because they don't want to track \"uniform\" patches\n",
    "    * They also measured the eigenvalues in \"highly textured\" regions\n",
    "    * This produced an upper bound\n",
    "        * The threshold has to be smaller than this because they don't want to track miss the \"highly textured\" patches\n",
    "    * They then picked a threshold in between\n",
    "        * Said the bounds are well apart\n",
    "        * Said the threshold is \"not critical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Lucas-Kanade method to track the motion of a point in a video\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the video\n",
    "cap = cv.VideoCapture('videos/vtest.avi')\n",
    "\n",
    "# Read the first frame\n",
    "ret, first_frame = cap.read()\n",
    "\n",
    "# Convert the first frame to grayscale\n",
    "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a zero array for the mask\n",
    "mask = np.zeros_like(first_frame)\n",
    "\n",
    "# Use Harris corner detector to find 20 best corners and track all of them\n",
    "corners = cv.goodFeaturesToTrack(prev_gray, 20, 0.01, 10)\n",
    "\n",
    "# Create an array to store the previous points\n",
    "prev_points = corners\n",
    "\n",
    "# Create a mask for the drawing\n",
    "mask = np.zeros_like(first_frame)\n",
    "\n",
    "# Read the video frame by frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate the optical flow using Lucas-Kanade method\n",
    "    next_points, status, error = cv.calcOpticalFlowPyrLK(prev_gray, gray, prev_points, None)\n",
    "\n",
    "    # Select the good points\n",
    "    good_new = next_points[status == 1]\n",
    "    good_prev = prev_points[status == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, prev) in enumerate(zip(good_new, good_prev)):\n",
    "        x_new, y_new = map(int, new.ravel())\n",
    "        x_prev, y_prev = map(int, prev.ravel())\n",
    "        mask = cv.line(mask, (x_new, y_new), (x_prev, y_prev), (0, 255, 0), 3)\n",
    "        frame = cv.circle(frame, (x_new, y_new), 8, (0, 0, 255), -1)\n",
    "    img = cv.add(frame, mask)\n",
    "\n",
    "    # Show the result\n",
    "    cv.imshow('frame', img)\n",
    "\n",
    "    # Break the loop\n",
    "    if cv.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "    # Update the previous frame and previous points\n",
    "    prev_gray = gray.copy()\n",
    "    prev_points = good_new.reshape(-1, 1, 2)\n",
    "    \n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all the windows\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1) # extra waitKey sometimes needed to close the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Obvious not perfect\n",
    "- Features that are \"good to track\" may be features that don't move\n",
    "- Features that are \"bad to track\" may be features that move a lot\n",
    "- During occlusions, the features may start tracking something else\n",
    "\n",
    "### Limitations\n",
    "- Not invariant to affine transformations\n",
    "- Not invariant to changes in brightness and contrast\n",
    "- Lots of parameters to tune\n",
    "- Designed for grayscale images (videos)\n",
    "    - Doesn't use color information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Optical Flow\n",
    "\n",
    "* Lucas-Kanade is based on tracking \"features\" (corners, edges, keypoints, etc.)\n",
    "* Dense optical flow is based on tracking every pixel in the image\n",
    "* This is computationally expensive, but can be useful in some applications\n",
    "* The most common algorithm for dense optical flow is the Farneback method\n",
    "\n",
    "## Farneback Method\n",
    "* The Farneback method is based on polynomial expansion of the image\n",
    "* It works by approximating the image motion with polynomial expansion\n",
    "* It then solves for the polynomial coefficients that minimize the difference between the two frames\n",
    "* This gives us two different types of information about each pixel\n",
    "    * The displacement is a vector\n",
    "    * Much like a gradient, we can think about the magnitude and angle\n",
    "    * Could represent the direction (angle) as color\n",
    "    * Could represent the magnitude as intensity or opacity\n",
    "* It is computationally efficient and can be used for real-time applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os \n",
    "\n",
    "cap = cv.VideoCapture(cv.samples.findFile(\"videos/vtest.avi\"))\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Show the frames side by side\n",
    "    two_frames = cv.resize(frame2, (640, 480))\n",
    "    bgr = cv.resize(bgr, (640, 480))\n",
    "    numpy_horizontal = np.hstack((two_frames, bgr))\n",
    "    cv.imshow('Side By Side', numpy_horizontal)    \n",
    "    \n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv.imwrite('opticalfb.png', frame2)\n",
    "        cv.imwrite('opticalhsv.png', bgr)\n",
    "    prvs = next\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1) # Will not close window on a Mac without this line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
