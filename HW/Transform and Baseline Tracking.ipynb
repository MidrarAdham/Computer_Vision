{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - Pixel Transform:\n",
    "\n",
    "\n",
    "Select an image of your choice to demonstrate performing the following pixel transforms. For each transform show the original image next to the transformed image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing modules and selecting an Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "img = cv2.imread('../sample_data/uv.JPG')\n",
    "# Convert from BGR to RGB:\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_plots(img1, img2):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(10,10))\n",
    "    fig.tight_layout()\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].imshow(img1)\n",
    "    ax[1].set_title('Modified Image')\n",
    "    ax[1].imshow(img2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. **Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shape:\n",
    "img_shape = img.shape\n",
    "\n",
    "height = img_shape[0]\n",
    "width = img_shape[1]\n",
    "\n",
    "# Create an indentity matrix\n",
    "identity_matrix = np.eye(3)\n",
    "# Modify the translation part:\n",
    "identity_matrix[0,2] = 200 # x\n",
    "identity_matrix[1,2] = 250 # y\n",
    "\n",
    "imageWarped = cv2.warpPerspective(img,identity_matrix,(width, height))\n",
    "para_plots(img1=img, img2=imageWarped)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Rotation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 10.0*(np.pi/180.0)\n",
    "\n",
    "x_axis_rotation_point = 200 # move the image right or left\n",
    "y_axis_rotation_point = 0   # move the image up or down\n",
    "translation = [0.0,0.0,1.0]\n",
    "\n",
    "M = np.array([[np.cos(theta),-np.sin(theta), x_axis_rotation_point],\n",
    "              [np.sin(theta),np.cos(theta), y_axis_rotation_point],\n",
    "              translation\n",
    "              ])\n",
    "\n",
    "imageWarped = cv2.warpPerspective(img,M,(width,height))\n",
    "para_plots(img1=img,img2=imageWarped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.6 # resize the image\n",
    "b = 0.0 # rotate the image\n",
    "tx = 0  # move the image along the x-axis\n",
    "ty = 0  # move the image along the y-axis\n",
    "\n",
    "# scaled = cv2.resize(img,(int(width*10),int(height*10)),interpolation=cv2.INTER_NEAREST)\n",
    "M = np.array([[a,-b,tx],[b,a,ty],translation])\n",
    "x = cv2.warpPerspective(img,M,(width,height))\n",
    "para_plots(img1=img,img2=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Rotation, scaling, $\\&$ Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.6 # resize the image (scaling)\n",
    "rotate = 0.1 # rotate the image\n",
    "tx = 250  # move the image along the x-axis\n",
    "ty = 500  # move the image along the y-axis\n",
    "\n",
    "M = np.array([[scale,-rotate,tx],[rotate,scale,ty],translation])\n",
    "x = cv2.warpPerspective(img,M,(width,height))\n",
    "para_plots(img1=img,img2=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Affine:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a00 =  0.9\n",
    "a01 =  0.1\n",
    "a10 = -0.4\n",
    "a11 =  0.6\n",
    "tx = 0\n",
    "ty = 800\n",
    "\n",
    "M = np.array([[a00,a01,tx],[a10,a11,ty],[0.0,0.0,1.0]])\n",
    "print(M)\n",
    "\n",
    "imageWarped = cv2.warpPerspective(img,M,(width,height))\n",
    "\n",
    "para_plots(img1=img, img2=imageWarped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Projective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a00 =  1.0\n",
    "a01 =  0.0\n",
    "a10 =  0.0\n",
    "a11 =  1.0\n",
    "tx  = 250\n",
    "ty  = 400\n",
    "a20 = 0.0001\n",
    "a21 = 0.0001\n",
    "a22 = 1.0\n",
    "\n",
    "M = np.array([[a00,a01,tx],[a10,a11,ty],[a20,a21,a22]])\n",
    "print(M)\n",
    "\n",
    "imageWarped = cv2.warpPerspective(img,M,(width, height))\n",
    "para_plots(img1=img, img2=imageWarped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Perspective Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine the best projective transform that restores the image to a picture that is centered with the optical axis and does not contain any rotation, changes to aspect ratio, skew, or keystone distortion. You are welcome to use OpenCV and other tools to help with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the image:\n",
    "\n",
    "grid_img = cv2.imread('../sample_data/perspective_transform.jpg')\n",
    "grid_img = cv2.cvtColor(grid_img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw circles on the corners\n",
    "def draw_circle(img,coordinates, color):\n",
    "    for i in range(len(coordinates)):\n",
    "        cv2.circle(img, center=coordinates[i], radius=30,color=color, thickness=-1)\n",
    "plt.imshow(grid_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want to get a projection of the whole image. I only want to get the reflection of the box. Therefore, I specify four points (the corner of the Godiva box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_left_corner = (120,930)\n",
    "top_right_corner = (2910,820)\n",
    "bottom_left_corner = (110,3050)\n",
    "bottom_right_corner = (2990,3030)\n",
    "red = (255,0,0)\n",
    "blue = (0,0,255)\n",
    "img_cp = grid_img.copy()\n",
    "src_coordinates = [top_left_corner,top_right_corner,bottom_left_corner,bottom_right_corner]\n",
    "draw_circle(img=img_cp, coordinates=src_coordinates, color=red)\n",
    "plt.imshow(img_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# manually mapping between source and dest.\n",
    "\n",
    "top_left_corner_dest = (int(top_left_corner[0]/4), int(top_left_corner[1]/4))\n",
    "top_right_corner_dest = (int(3 * top_right_corner[0]/4), int(top_right_corner[1]/4))\n",
    "bottom_left_corner_dest = (int(3 * bottom_left_corner[0]/4), int(3 * bottom_left_corner[1]/4))\n",
    "bottom_right_corner_dest = (2182, 2287)\n",
    "\n",
    "dest_coordinates = [top_left_corner_dest,top_right_corner_dest,bottom_left_corner_dest, bottom_right_corner_dest]\n",
    "draw_circle(img=img_cp,coordinates=dest_coordinates,color=blue)\n",
    "plt.imshow(img_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_points = np.float32(src_coordinates)\n",
    "dest_points = np.float32(dest_coordinates)\n",
    "output_width = max(top_right_corner_dest[0], bottom_right_corner_dest[0])\n",
    "output_height = max(top_left_corner_dest[0], bottom_left_corner_dest[1])\n",
    "perspective_matrix = cv2.getPerspectiveTransform(src_points, dest_points)\n",
    "results = cv2.warpPerspective(grid_img,perspective_matrix,(output_width,output_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. State the projective transform matrix (3 x 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perspective_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe for how you determined the projective transform and provide a justification for any design decisions you made (e.g, selection of any parameter values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I choose an image with a box in it. The box is what I wanted to project from the world to the output. Therefore, my matrix are a set of four points I specified as the corners of the box (top-right, top-left, bottom-right, and bottom-left). Each corner coordinates is red-circled. To map these four corners in the output image, I scaled the input points. I wanted to apply L1 norm or L2 but the output image did not reflect correctly. I adjusted the scaled matrix values manually, and after several trials and errors, I think the output image projected the box within the input image appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Original and restored images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_plots(img1=grid_img, img2=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please ignore the following. A failed maethod to use L1 norm to get the projected points on the other side of the plane**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# width_top_corners = np.sqrt(((top_left_corner[0] - top_right_corner[0]) ** 2) +\n",
    "#                             ((top_left_corner[1] - top_right_corner[1]) ** 2)\n",
    "#                             )\n",
    "\n",
    "# width_bottom_corners = np.sqrt(((bottom_left_corner[0] - bottom_right_corner[0]) ** 2) +\n",
    "#                                ((bottom_left_corner[1] - bottom_right_corner[1]) ** 2)\n",
    "#                                )\n",
    "\n",
    "# max_width = max(int(width_top_corners), int(width_bottom_corners))\n",
    "\n",
    "# height_left_corners = np.sqrt(((top_left_corner[0] - bottom_left_corner[0]) ** 2) +\n",
    "#                              ((top_left_corner[1] - bottom_left_corner[1]) ** 2)\n",
    "#                              )\n",
    "\n",
    "# height_right_corners = np.sqrt(((top_right_corner[0] - bottom_right_corner[0]) ** 2) +\n",
    "#                                 ((top_right_corner[1] - bottom_right_corner[1]) ** 2)\n",
    "#                                 )\n",
    "# max_height = max(int(height_left_corners), int(height_right_corners))\n",
    "\n",
    "# src_points = np.float32(src_coordinates)\n",
    "# dest_points = np.float32([[0,0],\n",
    "#                           [0, max_height],\n",
    "#                           [max_width, max_height],\n",
    "#                           [max_width - 1,0]])\n",
    "# perspective_matrix = cv2.getPerspectiveTransform(src_points, dest_points)\n",
    "# results = cv2.warpPerspective(grid_img, perspective_matrix,(width, height), flags=cv2.INTER_LINEAR)\n",
    "# plt.imshow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Object Tracking\n",
    "\n",
    "1. **Explain why you chose this video and this object for tracking.**\n",
    "\n",
    "    - The video is easy to process since it has the same background over the duration of the video.\n",
    "    - The vaccum moves in one direction (vertically), so it is easy to track.\n",
    "    - Upon completion of this homework, I realized that tracking an object vertically is actually harder than I thought. The reason is the object size changes\n",
    "    as it moves away from the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the first frame of the video, draw an overlayed boundary (e.g., a boundary box) around the object you plan to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_first_frame(frame, coords):\n",
    "    x = coords[0]\n",
    "    y = coords[1]\n",
    "    w = coords[2]\n",
    "    h = coords[3]\n",
    "    cv2.rectangle(frame,pt1=(x,y),pt2=(x+w,y+h),color=(255,0,0), thickness=5)\n",
    "    plt.imshow('first frame',frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Develop some method for tracking the object over the duration of the video. During each frame of the video, indicate where the object is with an overlayed boundary. You are welcome to use any method you like, whether it's something you developed or something someone else developed. Describe your object tracking algorithm, and why you chose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:23:05.321 Python[15939:2136193] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "capture = cv2.VideoCapture(\"../sample_data/object_tracking_2.mp4\")\n",
    "first_frame = True\n",
    "ret, frame = capture.read()\n",
    "fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "output = cv2.VideoWriter(\"Frame.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "# Manually choose the coordinates and dimesions of the vaccum in the first frame.\n",
    "x,y,w,h = 570,1880,500,610\n",
    "coords = [x,y,w,h]\n",
    "# Specify the Region of Interest (ROI)\n",
    "roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "# Starts dispalying the video frame by frame.\n",
    "while capture.isOpened():\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if first_frame:\n",
    "        display_first_frame(frame=frame, coords=coords)\n",
    "        first_frame = False\n",
    "        \n",
    "    img_frame = frame.copy()\n",
    "    # Change the frame color so I can mask certain color scales off of the object I am tracking\n",
    "    img_frame = cv2.cvtColor(img_frame, cv2.COLOR_BGR2XYZ)\n",
    "    '''\n",
    "    specify the range of colors we're masking. The vaccum is black with minimal variations of other colors.\n",
    "    So, I mask the black and the other colors that are a bit off the black color scale.\n",
    "    '''\n",
    "    mask = cv2.inRange(img_frame,lowerb=(0,0,0),upperb=(70,60,80))\n",
    "    # Grab the contours for the masked object\n",
    "    contours, _ = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    '''\n",
    "    The contours are list of lists. I iterate over each one and grab the coordinates and dimensions of each one.\n",
    "    I also grab the area of each contour. The area is used to limit the rectangles drawn around the object, as well as the space surrounding the object.\n",
    "    '''\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 1e3:\n",
    "            cv2.rectangle(frame,pt1=(x,y),pt2=(x+w,y+h),color=(255,0,0),thickness=3)\n",
    "\n",
    "    output.write(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Describe how well the tracking algorithm performed. Did it meet all of your expectations, or did it fail to track at some points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Upload the modified video with the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources used for this HW:\n",
    "    1- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
